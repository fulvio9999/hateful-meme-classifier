{"cells":[{"cell_type":"code","execution_count":1,"id":"18f1a27a","metadata":{"id":"18f1a27a"},"outputs":[{"name":"stderr","output_type":"stream","text":["d:\\CONDA_ENV\\deep_learning\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n","  from .autonotebook import tqdm as notebook_tqdm\n","d:\\CONDA_ENV\\deep_learning\\lib\\site-packages\\transformers\\generation_utils.py:24: FutureWarning: Importing `GenerationMixin` from `src/transformers/generation_utils.py` is deprecated and will be removed in Transformers v5. Import as `from transformers import GenerationMixin` instead.\n","  warnings.warn(\n","d:\\CONDA_ENV\\deep_learning\\lib\\site-packages\\torchaudio\\backend\\utils.py:74: UserWarning: No audio backend is available.\n","  warnings.warn(\"No audio backend is available.\")\n","Xformers is not installed correctly. If you want to use memory_efficient_attention to accelerate training use the following command to install Xformers\n","pip install xformers.\n"]}],"source":["import matplotlib.image as im\n","import json\n","import pandas as pd\n","import tqdm\n","import torch\n","import torch.nn as nn\n","import torch.optim as optim\n","import matplotlib.image as im\n","import json\n","import pandas as pd\n","import os\n","import json\n","import torch\n","import torchvision.transforms as transforms\n","import numpy as np\n","import matplotlib.pyplot as plt\n","\n","from sklearn.metrics import ConfusionMatrixDisplay, classification_report, confusion_matrix\n","from PIL import Image\n","from torch.utils.data import Dataset, DataLoader\n","from transformers import BertTokenizer, BertModel\n","from tqdm import tqdm\n","from sentence_transformers import SentenceTransformer\n","from transformers import *\n","from transformers import ViTImageProcessor, ViTModel\n","from sentence_transformers import SentenceTransformer\n"]},{"cell_type":"markdown","id":"bb1a4970","metadata":{"id":"bb1a4970"},"source":["# Metodi Gestione Immagini"]},{"cell_type":"code","execution_count":4,"id":"1bf8d9d1","metadata":{"id":"1bf8d9d1"},"outputs":[],"source":["def apri(file_path):\n","    # Definisci il percorso del file JSONL\n","\n","\n","    # Lista per memorizzare i record JSON analizzati\n","    json_records = []\n","\n","    # Apri il file JSONL in modalità lettura\n","    with open(file_path, \"r\") as file:\n","        # Leggi il file riga per riga\n","        for line in file:\n","            # Analizza il record JSON della riga corrente e lo aggiungi alla lista\n","            json_record = json.loads(line.strip())\n","            json_records.append(json_record)\n","\n","    return pd.DataFrame(json_records)"]},{"cell_type":"code","execution_count":5,"id":"7364ec76","metadata":{"id":"7364ec76"},"outputs":[],"source":["train = apri(\"train.jsonl\")\n","\n","train_pos = train[train[\"label\"] == 0].copy()\n","train_neg = train[train[\"label\"] == 1].copy()\n","\n","n_pos = len(train_pos)\n","n_neg = len(train_neg)\n","\n","n = n_pos\n","if(n_neg < n_pos):\n","    n= n_neg\n","\n","result = pd.concat([train_pos[0:n], train_neg[0:n]])\n","result.to_json('train_ridotto.jsonl', orient='records', lines=True)"]},{"cell_type":"markdown","id":"596abef7","metadata":{"id":"596abef7"},"source":["# FUSION AND CLASSIFICATION"]},{"cell_type":"code","execution_count":2,"id":"ec8e25dc","metadata":{"id":"ec8e25dc","outputId":"53a6ef37-74f5-4ba7-c01c-b7779444229d"},"outputs":[{"name":"stderr","output_type":"stream","text":["loading configuration file preprocessor_config.json from cache at C:\\Users\\fulvi/.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\7cbdb7ee3a6bcdf99dae654893f66519c480a0f8\\preprocessor_config.json\n","size should be a dictionary on of the following set of keys: ({'height', 'width'}, {'shortest_edge'}, {'shortest_edge', 'longest_edge'}, {'longest_edge'}), got 224. Converted to {'height': 224, 'width': 224}.\n","Image processor ViTImageProcessor {\n","  \"do_normalize\": true,\n","  \"do_rescale\": true,\n","  \"do_resize\": true,\n","  \"image_mean\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"image_processor_type\": \"ViTImageProcessor\",\n","  \"image_std\": [\n","    0.5,\n","    0.5,\n","    0.5\n","  ],\n","  \"resample\": 2,\n","  \"rescale_factor\": 0.00392156862745098,\n","  \"size\": {\n","    \"height\": 224,\n","    \"width\": 224\n","  }\n","}\n","\n","loading configuration file config.json from cache at C:\\Users\\fulvi/.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\7cbdb7ee3a6bcdf99dae654893f66519c480a0f8\\config.json\n","Model config ViTConfig {\n","  \"_name_or_path\": \"google/vit-base-patch16-224-in21k\",\n","  \"architectures\": [\n","    \"ViTModel\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.0,\n","  \"encoder_stride\": 16,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.0,\n","  \"hidden_size\": 768,\n","  \"image_size\": 224,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-12,\n","  \"model_type\": \"vit\",\n","  \"num_attention_heads\": 12,\n","  \"num_channels\": 3,\n","  \"num_hidden_layers\": 12,\n","  \"patch_size\": 16,\n","  \"qkv_bias\": true,\n","  \"transformers_version\": \"4.32.0.dev0\"\n","}\n","\n","loading weights file pytorch_model.bin from cache at C:\\Users\\fulvi/.cache\\huggingface\\hub\\models--google--vit-base-patch16-224-in21k\\snapshots\\7cbdb7ee3a6bcdf99dae654893f66519c480a0f8\\pytorch_model.bin\n","All model checkpoint weights were used when initializing ViTModel.\n","\n","All the weights of ViTModel were initialized from the model checkpoint at google/vit-base-patch16-224-in21k.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use ViTModel for predictions without further training.\n","loading configuration file C:\\Users\\fulvi/.cache\\torch\\sentence_transformers\\sentence-transformers_all-mpnet-base-v2\\config.json\n","Model config MPNetConfig {\n","  \"_name_or_path\": \"C:\\\\Users\\\\fulvi/.cache\\\\torch\\\\sentence_transformers\\\\sentence-transformers_all-mpnet-base-v2\\\\\",\n","  \"architectures\": [\n","    \"MPNetForMaskedLM\"\n","  ],\n","  \"attention_probs_dropout_prob\": 0.1,\n","  \"bos_token_id\": 0,\n","  \"eos_token_id\": 2,\n","  \"hidden_act\": \"gelu\",\n","  \"hidden_dropout_prob\": 0.1,\n","  \"hidden_size\": 768,\n","  \"initializer_range\": 0.02,\n","  \"intermediate_size\": 3072,\n","  \"layer_norm_eps\": 1e-05,\n","  \"max_position_embeddings\": 514,\n","  \"model_type\": \"mpnet\",\n","  \"num_attention_heads\": 12,\n","  \"num_hidden_layers\": 12,\n","  \"pad_token_id\": 1,\n","  \"relative_attention_num_buckets\": 32,\n","  \"transformers_version\": \"4.32.0.dev0\",\n","  \"vocab_size\": 30527\n","}\n","\n","loading weights file C:\\Users\\fulvi/.cache\\torch\\sentence_transformers\\sentence-transformers_all-mpnet-base-v2\\pytorch_model.bin\n","All model checkpoint weights were used when initializing MPNetModel.\n","\n","All the weights of MPNetModel were initialized from the model checkpoint at C:\\Users\\fulvi/.cache\\torch\\sentence_transformers\\sentence-transformers_all-mpnet-base-v2\\.\n","If your task is similar to the task the model of the checkpoint was trained on, you can already use MPNetModel for predictions without further training.\n","loading file vocab.txt\n","loading file tokenizer.json\n","loading file added_tokens.json\n","loading file special_tokens_map.json\n","loading file tokenizer_config.json\n"]},{"name":"stdout","output_type":"stream","text":["Modelli Caricati\n"]}],"source":["processor = ViTImageProcessor.from_pretrained('google/vit-base-patch16-224-in21k')\n","model_img = ViTModel.from_pretrained('google/vit-base-patch16-224-in21k')\n","#model_img.requires_grad_= False\n","model_text = SentenceTransformer('all-mpnet-base-v2')\n","#model_text.requires_grad_= False\n","print(\"Modelli Caricati\")\n","\n","\n","model_img.requires_grad_= True\n","model_text.requires_grad_= True"]},{"cell_type":"code","execution_count":7,"id":"eee99f3d","metadata":{"id":"eee99f3d","outputId":"747eaac8-fb5d-43cd-874f-9958bc3e759d"},"outputs":[{"ename":"FileNotFoundError","evalue":"[Errno 2] No such file or directory: 'image/img/42953.png'","output_type":"error","traceback":["\u001b[1;31m---------------------------------------------------------------------------\u001b[0m","\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[1;32md:\\BACKUP\\BACKUP Pen-drive Fulvio\\FLV\\UNIVERSITA'\\MAGISTRALE\\UNIVERSITA' 2\\2° SEMESTRE\\ANALISI DI SOCIAL NETWORK E MEDIA\\PROGETTO\\progettoASNM\\modelv2.ipynb Cell 7\u001b[0m line \u001b[0;36m6\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BACKUP/BACKUP%20Pen-drive%20Fulvio/FLV/UNIVERSITA%27/MAGISTRALE/UNIVERSITA%27%202/2%C2%B0%20SEMESTRE/ANALISI%20DI%20SOCIAL%20NETWORK%20E%20MEDIA/PROGETTO/progettoASNM/modelv2.ipynb#W6sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m text \u001b[39m=\u001b[39m train[\u001b[39m'\u001b[39m\u001b[39mtext\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m.\u001b[39mtolist()\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BACKUP/BACKUP%20Pen-drive%20Fulvio/FLV/UNIVERSITA%27/MAGISTRALE/UNIVERSITA%27%202/2%C2%B0%20SEMESTRE/ANALISI%20DI%20SOCIAL%20NETWORK%20E%20MEDIA/PROGETTO/progettoASNM/modelv2.ipynb#W6sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m img \u001b[39m=\u001b[39m  train\u001b[39m.\u001b[39miloc[\u001b[39m0\u001b[39m][\u001b[39m\"\u001b[39m\u001b[39mimg\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m----> <a href='vscode-notebook-cell:/d%3A/BACKUP/BACKUP%20Pen-drive%20Fulvio/FLV/UNIVERSITA%27/MAGISTRALE/UNIVERSITA%27%202/2%C2%B0%20SEMESTRE/ANALISI%20DI%20SOCIAL%20NETWORK%20E%20MEDIA/PROGETTO/progettoASNM/modelv2.ipynb#W6sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m img \u001b[39m=\u001b[39m im\u001b[39m.\u001b[39;49mimread(\u001b[39mf\u001b[39;49m\u001b[39m\"\u001b[39;49m\u001b[39mimage/\u001b[39;49m\u001b[39m{\u001b[39;49;00mimg\u001b[39m}\u001b[39;49;00m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BACKUP/BACKUP%20Pen-drive%20Fulvio/FLV/UNIVERSITA%27/MAGISTRALE/UNIVERSITA%27%202/2%C2%B0%20SEMESTRE/ANALISI%20DI%20SOCIAL%20NETWORK%20E%20MEDIA/PROGETTO/progettoASNM/modelv2.ipynb#W6sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(img\u001b[39m.\u001b[39mshape)\n\u001b[0;32m      <a href='vscode-notebook-cell:/d%3A/BACKUP/BACKUP%20Pen-drive%20Fulvio/FLV/UNIVERSITA%27/MAGISTRALE/UNIVERSITA%27%202/2%C2%B0%20SEMESTRE/ANALISI%20DI%20SOCIAL%20NETWORK%20E%20MEDIA/PROGETTO/progettoASNM/modelv2.ipynb#W6sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m img \u001b[39m=\u001b[39m processor(images\u001b[39m=\u001b[39mimg, return_tensors\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mpt\u001b[39m\u001b[39m\"\u001b[39m)\n","File \u001b[1;32md:\\CONDA_ENV\\deep_learning\\lib\\site-packages\\matplotlib\\image.py:1563\u001b[0m, in \u001b[0;36mimread\u001b[1;34m(fname, format)\u001b[0m\n\u001b[0;32m   1556\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(fname, \u001b[39mstr\u001b[39m) \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(parse\u001b[39m.\u001b[39murlparse(fname)\u001b[39m.\u001b[39mscheme) \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   1557\u001b[0m     \u001b[39m# Pillow doesn't handle URLs directly.\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m     \u001b[39mraise\u001b[39;00m \u001b[39mValueError\u001b[39;00m(\n\u001b[0;32m   1559\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mPlease open the URL for reading and pass the \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1560\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39mresult to Pillow, e.g. with \u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1561\u001b[0m         \u001b[39m\"\u001b[39m\u001b[39m``np.array(PIL.Image.open(urllib.request.urlopen(url)))``.\u001b[39m\u001b[39m\"\u001b[39m\n\u001b[0;32m   1562\u001b[0m         )\n\u001b[1;32m-> 1563\u001b[0m \u001b[39mwith\u001b[39;00m img_open(fname) \u001b[39mas\u001b[39;00m image:\n\u001b[0;32m   1564\u001b[0m     \u001b[39mreturn\u001b[39;00m (_pil_png_to_float_array(image)\n\u001b[0;32m   1565\u001b[0m             \u001b[39mif\u001b[39;00m \u001b[39misinstance\u001b[39m(image, PIL\u001b[39m.\u001b[39mPngImagePlugin\u001b[39m.\u001b[39mPngImageFile) \u001b[39melse\u001b[39;00m\n\u001b[0;32m   1566\u001b[0m             pil_to_array(image))\n","File \u001b[1;32md:\\CONDA_ENV\\deep_learning\\lib\\site-packages\\PIL\\ImageFile.py:105\u001b[0m, in \u001b[0;36mImageFile.__init__\u001b[1;34m(self, fp, filename)\u001b[0m\n\u001b[0;32m    101\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdecodermaxblock \u001b[39m=\u001b[39m MAXBLOCK\n\u001b[0;32m    103\u001b[0m \u001b[39mif\u001b[39;00m is_path(fp):\n\u001b[0;32m    104\u001b[0m     \u001b[39m# filename\u001b[39;00m\n\u001b[1;32m--> 105\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfp \u001b[39m=\u001b[39m \u001b[39mopen\u001b[39;49m(fp, \u001b[39m\"\u001b[39;49m\u001b[39mrb\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfilename \u001b[39m=\u001b[39m fp\n\u001b[0;32m    107\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_exclusive_fp \u001b[39m=\u001b[39m \u001b[39mTrue\u001b[39;00m\n","\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'image/img/42953.png'"]}],"source":["train = apri(\"train.jsonl\")\n","\n","\n","text = train['text'].tolist()\n","img =  train.iloc[0][\"img\"]\n","img = im.imread(f\"image/{img}\")\n","print(img.shape)\n","img = processor(images=img, return_tensors=\"pt\")\n","img = img['pixel_values']\n","print(img.shape)\n","print(len(text))\n","\n","img_emb = model_img(img)\n","text_emb = model_text.encode(text[0])"]},{"cell_type":"code","execution_count":null,"id":"d06000cc","metadata":{"id":"d06000cc","outputId":"92194fab-3b7c-463d-8dbd-bff04bf9d1fc"},"outputs":[{"name":"stdout","output_type":"stream","text":["Size dell'embedding dell'immagine: (768,)\n","Size dell'embedding del testo: (768,)\n"]}],"source":["print(f\"Size dell'embedding dell'immagine: {img_emb.pooler_output.squeeze().detach().numpy().shape}\")\n","print(f\"Size dell'embedding del testo: {text_emb.shape}\")"]},{"cell_type":"markdown","id":"5ba2e290","metadata":{"id":"5ba2e290"},"source":["#### DATALOADER:"]},{"cell_type":"code","execution_count":3,"id":"53a22350","metadata":{"id":"53a22350"},"outputs":[],"source":["\n","\n","class MemeDataset(Dataset):\n","    def __init__(self, data_path, image_folder, transform=None):\n","        self.data = []\n","        self.image_folder = image_folder\n","        self.transform = transform\n","\n","        # Carica i dati dal file JSON\n","        with open(data_path, \"r\") as f:\n","            for line in f:\n","                entry = json.loads(line)\n","                self.data.append(entry)\n","\n","    def __len__(self):\n","        return len(self.data)\n","\n","    def __getitem__(self, idx):\n","        entry = self.data[idx]\n","        img_name = os.path.join(self.image_folder, entry[\"img\"])\n","        image = Image.open(img_name).convert(\"RGB\")\n","\n","        if self.transform:\n","            image = self.transform(image)\n","\n","        text = entry[\"text\"]\n","        label = entry[\"label\"]\n","\n","        return image, text, label\n","\n","\n","transform = transforms.Compose([\n","    transforms.Resize((224, 224)),  # Ridimensiona l'immagine a 224x224 pixel\n","    transforms.ToTensor(),          # Converti l'immagine in un tensore\n","    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # Normalizza i valori dei pixel\n","])\n"]},{"cell_type":"markdown","id":"daa0af40","metadata":{"id":"daa0af40"},"source":["#### MODEL:"]},{"cell_type":"code","execution_count":4,"id":"324dd0c3","metadata":{"id":"324dd0c3","outputId":"a182fb39-e50c-4520-ffdf-67dde10bf16a"},"outputs":[],"source":["class MemeClassifier(nn.Module):\n","    def __init__(self, size_emb, model_img, model_text, device):\n","        super(MemeClassifier, self).__init__()\n","        self.model_img = model_img\n","        self.model_text = model_text\n","        self.device = device\n","        \n","        # self.img_weight = nn.Linear(size_emb, size_emb)\n","        # self.text_weight = nn.Linear(size_emb, size_emb)\n","        \n","        self.fusion1_fc = nn.Linear(2*size_emb, 1024)\n","        self.fusion2_fc = nn.Linear(1024, 512)\n","        self.fusion3_fc = nn.Linear(512,128)\n","        self.fusion4_fc = nn.Linear(128,64)\n","        self.output_fc = nn.Linear(64, 1)\n","\n","\n","    def forward(self, image, text):\n","        image_embedding = self.model_img(image).pooler_output\n","        #print(\"image embedding shape: \", image_embedding.shape)\n","        text_embedding = self.model_text.encode(text)\n","        text_embedding_tensor = torch.from_numpy(text_embedding).to(self.device)\n","        #print(\"text embedding shape: \", text_embedding_tensor.shape)\n","        # img_weight  = self.img_weight(image_embedding)\n","        # text_weight = self.text_weight(text_embedding_tensor)\n","        # emb = img_weight + text_weight\n","        emb = torch.cat((image_embedding, text_embedding_tensor), 1)\n","        #print(\"fused emb shape: \", emb.shape)\n","        fused_hidden = nn.ReLU()(self.fusion1_fc(emb))\n","        fused_hidden = nn.ReLU()(self.fusion2_fc(fused_hidden))\n","        fused_hidden = nn.ReLU()(self.fusion3_fc(fused_hidden))\n","        fused_hidden = nn.ReLU()(self.fusion4_fc(fused_hidden))\n","        output = nn.Sigmoid()(self.output_fc(fused_hidden))\n","        return output #, img_weight, text_weight\n"]},{"cell_type":"markdown","id":"8e771e1e","metadata":{"id":"8e771e1e"},"source":["#### TRAIN LOOP:"]},{"cell_type":"code","execution_count":5,"id":"da368811","metadata":{"id":"da368811"},"outputs":[],"source":["def train_model(model, train_dataloader, criterion, optimizer, epochs, device, model_file):\n","    model.train()\n","    min_loss = float(\"inf\")\n","    stop = False\n","    for epoch in range(epochs):\n","        epoch_loss = 0.0\n","        # for batch in train_dataloader:\n","        pbar = tqdm(train_dataloader)\n","        for i, (images, texts, labels) in enumerate(pbar):\n","            images = images.to(device)\n","            # texts = texts.to(device)\n","            labels = labels.to(device)\n","            optimizer.zero_grad()\n","            outputs = model(images, texts)\n","\n","            if np.isnan(outputs.cpu().detach().numpy()[:,0][0]):\n","                print(f\"({texts})\")\n","                #print(f\"({np.min(im_emb)},{np.max(im_emb)})\")\n","                #print(f\"({np.min(text_emb)},{np.max(text_emb)})\")\n","                print(f\"{outputs})\")\n","                print(f\"({labels})\")\n","                #print(f\"{outputs})\")\n","                stop = True\n","\n","            else:\n","                loss = criterion(outputs.float(), labels.unsqueeze(1).float())\n","                loss.backward()\n","                optimizer.step()\n","                #total_loss += loss.item()\n","                pbar.set_postfix(BCE=loss.item())\n","                epoch_loss += loss.item()\n","                stop = False\n","        if stop:\n","            break\n","        \n","        epoch_loss /= len(train_dataloader)\n","        if epoch_loss < min_loss :\n","            min_loss = epoch_loss\n","            print(f\"Loss at epoch {epoch + 1}: {epoch_loss:.3f}\")\n","            torch.save(model.state_dict(), f\"{model_file}.pt\")\n","\n"]},{"cell_type":"code","execution_count":6,"id":"ba9299a5","metadata":{"id":"ba9299a5"},"outputs":[],"source":["# Iperparametri\n","size_emb = 768\n","learning_rate = 1e-4\n","epochs = 20\n","device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n","model_file = './results/vit_mpnet_6100_32_new'\n","data_path = \"train.jsonl\"\n","image_folder = \"data/\"\n","batch_size = 4 #16"]},{"cell_type":"markdown","id":"4e6b9d7f","metadata":{"id":"4e6b9d7f"},"source":["#### RUN TRAINING:"]},{"cell_type":"code","execution_count":11,"id":"3a853f59","metadata":{"id":"3a853f59","outputId":"cf90bcb5-133f-451c-e1fc-2a10079a5eff"},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:52<00:00,  3.08it/s, BCE=0.644]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 1: 0.647\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:51<00:00,  3.10it/s, BCE=0.382]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 2: 0.581\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:49<00:00,  3.14it/s, BCE=0.611]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 3: 0.558\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:48<00:00,  3.16it/s, BCE=0.251]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 4: 0.548\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:47<00:00,  3.18it/s, BCE=0.57] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 5: 0.539\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:48<00:00,  3.17it/s, BCE=0.237]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 6: 0.523\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:47<00:00,  3.17it/s, BCE=0.357]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 7: 0.500\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:47<00:00,  3.17it/s, BCE=0.711]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 8: 0.472\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:48<00:00,  3.15it/s, BCE=0.167]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 9: 0.432\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:46<00:00,  3.19it/s, BCE=0.695] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 10: 0.367\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:49<00:00,  3.13it/s, BCE=0.513] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 11: 0.290\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:49<00:00,  3.14it/s, BCE=0.0606]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 12: 0.220\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:49<00:00,  3.14it/s, BCE=0.0761] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 13: 0.172\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:47<00:00,  3.17it/s, BCE=0.206]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 14: 0.135\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:48<00:00,  3.16it/s, BCE=0.037]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 15: 0.105\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:48<00:00,  3.16it/s, BCE=0.0463] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 16: 0.075\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:47<00:00,  3.17it/s, BCE=0.00161] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 17: 0.071\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:48<00:00,  3.15it/s, BCE=0.0655]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 18: 0.057\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:48<00:00,  3.16it/s, BCE=0.0432]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 19: 0.053\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 532/532 [02:50<00:00,  3.12it/s, BCE=0.00621] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 20: 0.046\n"]}],"source":["dataset = MemeDataset(data_path, image_folder, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","model = MemeClassifier(size_emb, model_img, model_text, device)\n","model.to(device)\n","\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Addestramento del modello\n","train_model(model, dataloader, criterion, optimizer, epochs, device, model_file)"]},{"cell_type":"code","execution_count":14,"id":"da921da7","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:16<00:00, 30.39it/s]"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.55      0.67      0.61       250\n","           1       0.58      0.46      0.51       250\n","\n","    accuracy                           0.56       500\n","   macro avg       0.57      0.56      0.56       500\n","weighted avg       0.57      0.56      0.56       500\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Eval\n","model = MemeClassifier(size_emb, model_img, model_text, device)\n","model.load_state_dict(torch.load(f\"{model_file}.pt\"))\n","model.to(device)\n","model.eval()\n","\n","dataset_test = MemeDataset(\"dev.jsonl\", \"image/\", transform=transform)\n","dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n","\n","pred_labels = []\n","labels = []\n","pbar = tqdm(dataloader_test)\n","for step, (image, text, label) in enumerate(pbar):\n","\n","    image = image.to(device)\n","    #label = label.to(device)\n","    output = model(image, text)\n","    predictions = (output.cpu().detach().numpy() > 0.5).astype(int)\n","    pred_labels.extend(predictions)\n","    labels.extend(label)\n","\n","print(classification_report(labels, pred_labels))"]},{"cell_type":"markdown","id":"5def59f5","metadata":{},"source":["#### RUN TRAINING CON NORMALIZZAZIONE:"]},{"cell_type":"code","execution_count":null,"id":"ebbae9bd","metadata":{},"outputs":[],"source":["import torch.nn.functional as F\n","\n","class MemeClassifier(nn.Module):\n","    def __init__(self, size_emb, model_img, model_text, device):\n","        super(MemeClassifier, self).__init__()\n","        self.model_img = model_img\n","        self.model_text = model_text\n","        self.device = device\n","        self.fusion1_fc = nn.Linear(2 * size_emb, 1024)\n","        self.fusion2_fc = nn.Linear(1024, 512)\n","        self.fusion3_fc = nn.Linear(512, 128)\n","        self.fusion4_fc = nn.Linear(128, 64)\n","        self.output_fc = nn.Linear(64, 1)\n","\n","    def forward(self, image, text):\n","        image_embedding = self.model_img(image).pooler_output\n","        text_embedding = self.model_text.encode(text)\n","        text_embedding_tensor = torch.from_numpy(text_embedding).to(self.device)\n","\n","        # Normalize the image and text embeddings\n","        image_embedding = F.normalize(image_embedding, p=2, dim=1)\n","        text_embedding_tensor = F.normalize(text_embedding_tensor, p=2, dim=1)\n","\n","        # Concatenate the normalized embeddings\n","        emb = torch.cat((image_embedding, text_embedding_tensor), 1)\n","\n","        fused_hidden = nn.ReLU()(self.fusion1_fc(emb))\n","        fused_hidden = nn.ReLU()(self.fusion2_fc(fused_hidden))\n","        fused_hidden = nn.ReLU()(self.fusion3_fc(fused_hidden))\n","        fused_hidden = nn.ReLU()(self.fusion4_fc(fused_hidden))\n","        output = nn.Sigmoid()(self.output_fc(fused_hidden))\n","        return output\n"]},{"cell_type":"code","execution_count":null,"id":"f7b24772","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [22:49<00:00,  1.55it/s, BCE=0.861]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 1: 0.594\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [19:10<00:00,  1.85it/s, BCE=0.633]\n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 2: 0.552\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:18<00:00,  1.93it/s, BCE=0.596] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 3: 0.544\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:17<00:00,  1.94it/s, BCE=1.17]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 4: 0.533\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:06<00:00,  1.96it/s, BCE=0.452] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 5: 0.522\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:10<00:00,  1.95it/s, BCE=0.375] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 6: 0.508\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:03<00:00,  1.96it/s, BCE=0.304] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 7: 0.495\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:08<00:00,  1.95it/s, BCE=0.566] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 8: 0.476\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [19:03<00:00,  1.86it/s, BCE=0.527] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 9: 0.456\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:01<00:00,  1.97it/s, BCE=0.411] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 10: 0.436\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:03<00:00,  1.96it/s, BCE=0.136]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 11: 0.413\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:07<00:00,  1.95it/s, BCE=0.188]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 12: 0.393\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:01<00:00,  1.97it/s, BCE=0.0732] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 13: 0.367\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:00<00:00,  1.97it/s, BCE=0.289]   \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 14: 0.343\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:06<00:00,  1.96it/s, BCE=0.468]   \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 15: 0.330\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:31<00:00,  1.91it/s, BCE=0.183]   \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 16: 0.315\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:02<00:00,  1.96it/s, BCE=0.227]   \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 17: 0.302\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:05<00:00,  1.96it/s, BCE=0.00719] \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 18: 0.288\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [19:45<00:00,  1.79it/s, BCE=0.333]   \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 19: 0.274\n"]},{"name":"stderr","output_type":"stream","text":["100%|██████████| 2125/2125 [18:52<00:00,  1.88it/s, BCE=0.0154]  \n"]},{"name":"stdout","output_type":"stream","text":["Loss at epoch 20: 0.265\n"]}],"source":["dataset = MemeDataset(data_path, image_folder, transform=transform)\n","dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True)\n","\n","model = MemeClassifier(size_emb, model_img, model_text, device)\n","model.to(device)\n","\n","criterion = nn.BCELoss()\n","optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n","\n","# Addestramento del modello\n","train_model(model, dataloader, criterion, optimizer, epochs, device, model_file)"]},{"cell_type":"code","execution_count":null,"id":"be47782c","metadata":{},"outputs":[{"name":"stderr","output_type":"stream","text":["100%|██████████| 500/500 [00:52<00:00,  9.44it/s]"]},{"name":"stdout","output_type":"stream","text":["              precision    recall  f1-score   support\n","\n","           0       0.54      0.79      0.64       250\n","           1       0.61      0.32      0.42       250\n","\n","    accuracy                           0.56       500\n","   macro avg       0.57      0.56      0.53       500\n","weighted avg       0.57      0.56      0.53       500\n","\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["# Eval\n","model = MemeClassifier(size_emb, model_img, model_text, device)\n","model.load_state_dict(torch.load(f\"{model_file}.pt\"))\n","model.to(device)\n","model.eval()\n","\n","dataset_test = MemeDataset(\"dev.jsonl\", \"data/\", transform=transform)\n","dataloader_test = DataLoader(dataset_test, batch_size=1, shuffle=False)\n","\n","pred_labels = []\n","labels = []\n","pbar = tqdm(dataloader_test)\n","for step, (image, text, label) in enumerate(pbar):\n","\n","    image = image.to(device)\n","    #label = label.to(device)\n","    output = model(image, text)\n","    predictions = (output.cpu().detach().numpy() > 0.5).astype(int)\n","    pred_labels.extend(predictions)\n","    labels.extend(label)\n","\n","print(classification_report(labels, pred_labels))"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.12"}},"nbformat":4,"nbformat_minor":5}
